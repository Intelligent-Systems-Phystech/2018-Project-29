@InProceedings{DBLP:conf/pkdd/DlikmanL16,
	author    = {Alexander Dlikman and Mark Last},
	title     = {Using Machine Learning Methods and Linguistic Features in Single-Document Extractive Summarization},
	booktitle = {Proceedings of the Workshop on Interactions between Data Mining and Natural Language Processing, {DMNLP} 2016, co-located with the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases, {ECML-PKDD} 2016, Riva del Garda, Italy, September 23, 2016.},
	year      = {2016},
	pages     = {1--8},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl    = {https://dblp.org/rec/bib/conf/pkdd/DlikmanL16},
	crossref  = {DBLP:conf/pkdd/2016dmnlp},
	timestamp = {Tue, 02 Aug 2016 16:20:44 +0200},
	url       = {http://ceur-ws.org/Vol-1646/paper1.pdf},
}

@InProceedings{Ren:2017:LCS:3077136.3080792,
	author    = {Ren, Pengjie and Chen, Zhumin and Ren, Zhaochun and Wei, Furu and Ma, Jun and de Rijke, Maarten},
	title     = {Leveraging Contextual Sentence Relations for Extractive Summarization Using a Neural Attention Model},
	booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	year      = {2017},
	series    = {SIGIR '17},
	pages     = {95--104},
	address   = {New York, NY, USA},
	publisher = {ACM},
	acmid     = {3080792},
	doi       = {10.1145/3077136.3080792},
	isbn      = {978-1-4503-5022-8},
	keywords  = {contextual sentence relation, extractive summarization, neural network},
	location  = {Shinjuku, Tokyo, Japan},
	numpages  = {10},
	url       = {http://doi.acm.org/10.1145/3077136.3080792},
}

@InProceedings{Wan:2011:UBI:2002472.2002659,
	author    = {Wan, Xiaojun},
	title     = {Using Bilingual Information for Cross-language Document Summarization},
	booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1},
	year      = {2011},
	series    = {HLT '11},
	pages     = {1546--1555},
	address   = {Stroudsburg, PA, USA},
	publisher = {Association for Computational Linguistics},
	acmid     = {2002659},
	isbn      = {978-1-932432-87-9},
	location  = {Portland, Oregon},
	numpages  = {10},
	url       = {http://dl.acm.org/citation.cfm?id=2002472.2002659},
}

@InProceedings{P18-1061,
	author    = {Zhou, Qingyu and Yang, Nan and Wei, Furu and Huang, Shaohan and Zhou, Ming and Zhao, Tiejun},
	title     = {Neural Document Summarization by Jointly Learning to Score and Select Sentences},
	booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	year      = {2018},
	pages     = {654--663},
	publisher = {Association for Computational Linguistics},
	location  = {Melbourne, Australia},
	url       = {http://aclweb.org/anthology/P18-1061},
}

@Article{DBLP:journals/corr/NallapatiZZ16,
	author        = {Ramesh Nallapati and Feifei Zhai and Bowen Zhou},
	title         = {SummaRuNNer: {A} Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents},
	journal       = {CoRR},
	year          = {2016},
	volume        = {abs/1611.04230},
	archiveprefix = {arXiv},
	bibsource     = {dblp computer science bibliography, https://dblp.org},
	biburl        = {https://dblp.org/rec/bib/journals/corr/NallapatiZZ16},
	eprint        = {1611.04230},
	timestamp     = {Mon, 13 Aug 2018 16:47:35 +0200},
	url           = {http://arxiv.org/abs/1611.04230},
}

@InProceedings{Wan:2010:CDS:1858681.1858775,
	author    = {Wan, Xiaojun and Li, Huiying and Xiao, Jianguo},
	title     = {Cross-language Document Summarization Based on Machine Translation Quality Prediction},
	booktitle = {Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics},
	year      = {2010},
	series    = {ACL '10},
	pages     = {917--926},
	address   = {Stroudsburg, PA, USA},
	publisher = {Association for Computational Linguistics},
	acmid     = {1858775},
	location  = {Uppsala, Sweden},
	numpages  = {10},
	url       = {http://dl.acm.org/citation.cfm?id=1858681.1858775},
}

@Article{DBLP:journals/corr/KleinKDSR17,
	author        = {Guillaume Klein and Yoon Kim and Yuntian Deng and Jean Senellart and Alexander M. Rush},
	title         = {OpenNMT: Open-Source Toolkit for Neural Machine Translation},
	journal       = {CoRR},
	year          = {2017},
	volume        = {abs/1701.02810},
	archiveprefix = {arXiv},
	bibsource     = {dblp computer science bibliography, https://dblp.org},
	biburl        = {https://dblp.org/rec/bib/journals/corr/KleinKDSR17},
	eprint        = {1701.02810},
	timestamp     = {Mon, 13 Aug 2018 16:46:49 +0200},
	url           = {http://arxiv.org/abs/1701.02810},
}

@InProceedings{Jain2017a,
	author        = {Aditya Jain and Divij Bhatia and Manish K Thakur},
	title         = {Extractive Text Summarization Using Word Vector Embedding},
	booktitle     = {2017 International Conference on Machine Learning and Data Science ({MLDS})},
	year          = {2017},
	month         = {dec},
	publisher     = {{IEEE}},
	__markedentry = {[Silya:6]},
	doi           = {10.1109/mlds.2017.12},
}

@Article{Zhang2018,
	author      = {Chi Zhang and Shagan Sah and Thang Nguyen and Dheeraj Peri and Alexander Loui and Carl Salvaggio and Raymond Ptucha},
	title       = {Semantic Sentence Embeddings for Paraphrasing and Text Summarization},
	abstract    = {This paper introduces a sentence to vector encoding framework suitable for advanced natural language processing. Our latent representation is shown to encode sentences with common semantic information with similar vector representations. The vector representation is extracted from an encoder-decoder model which is trained on sentence paraphrase pairs. We demonstrate the application of the sentence representations for two different tasks -- sentence paraphrasing and paragraph summarization, making it attractive for commonly used recurrent frameworks that process text. Experimental results help gain insight how vector representations are suitable for advanced language embedding.},
	date        = {2018-09-26},
	eprint      = {http://arxiv.org/abs/1809.10267v1},
	eprintclass = {cs.CL},
	eprinttype  = {arXiv},
	file        = {:http\://arxiv.org/pdf/1809.10267v1:PDF},
	keywords    = {cs.CL, cs.AI},
}

@Article{Mehta2018,
	author      = {Parth Mehta and Prasenjit Majumder},
	title       = {Exploiting local and global performance of candidate systems for aggregation of summarization techniques},
	abstract    = {With an ever growing number of extractive summarization techniques being proposed, there is less clarity then ever about how good each system is compared to the rest. Several studies highlight the variance in performance of these systems with change in datasets or even across documents within the same corpus. An effective way to counter this variance and to make the systems more robust could be to use inputs from multiple systems when generating a summary. In the present work, we define a novel way of creating such ensemble by exploiting similarity between the content of candidate summaries to estimate their reliability. We define GlobalRank which captures the performance of a candidate system on an overall corpus and LocalRank which estimates its performance on a given document cluster. We then use these two scores to assign a weight to each individual systems, which is then used to generate the new aggregate ranking. Experiments on DUC2003 and DUC 2004 datasets show a significant improvement in terms of ROUGE score, over existing sate-of-art techniques.},
	date        = {2018-09-07},
	eprint      = {http://arxiv.org/abs/1809.02343v1},
	eprintclass = {cs.IR},
	eprinttype  = {arXiv},
	file        = {:http\://arxiv.org/pdf/1809.02343v1:PDF},
	keywords    = {cs.IR, cs.AI, cs.CL},
}

@Article{Wan2018,
	author    = {Xiaojun Wan and Fuli Luo and Xue Sun and Songfang Huang and Jin-ge Yao},
	title     = {Cross-language document summarization via extraction and ranking of multiple summaries},
	journal   = {Knowledge and Information Systems},
	year      = {2018},
	month     = {jan},
	doi       = {10.1007/s10115-018-1152-7},
	publisher = {Springer Nature},
}

@InProceedings{Mihalcea_alanguage,
	author    = {Rada Mihalcea and Paul Tarau},
	title     = {A language independent algorithm for single and multiple document summarization},
	booktitle = {In Proceedings of IJCNLP?2005},
}

@Article{Zhang2018a,
	author      = {Xingxing Zhang and Mirella Lapata and Furu Wei and Ming Zhou},
	title       = {Neural Latent Extractive Document Summarization},
	abstract    = {Extractive summarization models require sentence-level labels, which are usually created heuristically (e.g., with rule-based methods) given that most summarization datasets only have document-summary pairs. Since these labels might be suboptimal, we propose a latent variable extractive model where sentences are viewed as latent variables and sentences with activated variables are used to infer gold summaries. During training the loss comes \emph{directly} from gold summaries. Experiments on the CNN/Dailymail dataset show that our model improves over a strong extractive baseline trained on heuristically approximated labels and also performs competitively to several recent models.},
	date        = {2018-08-22},
	eprint      = {http://arxiv.org/abs/1808.07187v2},
	eprintclass = {cs.CL},
	eprinttype  = {arXiv},
	file        = {:http\://arxiv.org/pdf/1808.07187v2:PDF},
	keywords    = {cs.CL, cs.AI, cs.LG},
}

@InProceedings{N16-1012,
	author    = {Chopra, Sumit and Auli, Michael and Rush, Alexander M.},
	title     = {Abstractive Sentence Summarization with Attentive Recurrent Neural Networks},
	booktitle = {Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	year      = {2016},
	pages     = {93--98},
	publisher = {Association for Computational Linguistics},
	doi       = {10.18653/v1/N16-1012},
	location  = {San Diego, California},
	url       = {http://www.aclweb.org/anthology/N16-1012},
}

@Article{See2017,
	author      = {Abigail See and Peter J. Liu and Christopher D. Manning},
	title       = {Get To The Point: Summarization with Pointer-Generator Networks},
	abstract    = {Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.},
	date        = {2017-04-14},
	eprint      = {http://arxiv.org/abs/1704.04368v2},
	eprintclass = {cs.CL},
	eprinttype  = {arXiv},
	file        = {:http\://arxiv.org/pdf/1704.04368v2:PDF},
	keywords    = {cs.CL},
}

@Article{Wu2016,
	author      = {Yonghui Wu and Mike Schuster and Zhifeng Chen and Quoc V. Le and Mohammad Norouzi and Wolfgang Macherey and Maxim Krikun and Yuan Cao and Qin Gao and Klaus Macherey and Jeff Klingner and Apurva Shah and Melvin Johnson and Xiaobing Liu and ?ukasz Kaiser and Stephan Gouws and Yoshikiyo Kato and Taku Kudo and Hideto Kazawa and Keith Stevens and George Kurian and Nishant Patil and Wei Wang and Cliff Young and Jason Smith and Jason Riesa and Alex Rudnick and Oriol Vinyals and Greg Corrado and Macduff Hughes and Jeffrey Dean},
	title       = {Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation},
	abstract    = {Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units ("wordpieces") for both input and output. This method provides a good balance between the flexibility of "character"-delimited models and the efficiency of "word"-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60% compared to Google's phrase-based production system.},
	date        = {2016-09-26},
	eprint      = {http://arxiv.org/abs/1609.08144v2},
	eprintclass = {cs.CL},
	eprinttype  = {arXiv},
	file        = {:http\://arxiv.org/pdf/1609.08144v2:PDF},
	keywords    = {cs.CL, cs.AI, cs.LG},
}

@InCollection{Singh2017,
	author    = {Shashi Pal Singh and Ajai Kumar and Hemant Darbari and Anshika Rastogi and Shikha Jain and Nisheeth Joshi},
	title     = {Building Machine Learning System with Deep Neural Network for Text Processing},
	booktitle = {Information and Communication Technology for Intelligent Systems ({ICTIS} 2017) - Volume 2},
	publisher = {Springer International Publishing},
	year      = {2017},
	pages     = {497--504},
	month     = {aug},
	doi       = {10.1007/978-3-319-63645-0_56},
}

@InProceedings{Zhang2018b,
	author    = {Yong Zhang and Jinzhi Liao and Jiuyang Tang and Weidong Xiao and Yuheng Wang},
	title     = {Extractive Document Summarization Based on Hierarchical {GRU}},
	booktitle = {2018 International Conference on Robots {\&} Intelligent System ({ICRIS})},
	year      = {2018},
	month     = {may},
	publisher = {{IEEE}},
	doi       = {10.1109/icris.2018.00092},
}

@InCollection{Pontes2018,
	author    = {Elvys Linhares Pontes and St{\'{e}}phane Huet and Juan-Manuel Torres-Moreno and Andr{\'{e}}a Carneiro Linhares},
	title     = {Cross-Language Text Summarization Using Sentence and Multi-Sentence Compression},
	booktitle = {Natural Language Processing and Information Systems},
	publisher = {Springer International Publishing},
	year      = {2018},
	pages     = {467--479},
	doi       = {10.1007/978-3-319-91947-8_48},
}

@Misc{wang2018learning,
	author = {Yau-Shian Wang and Hung-Yi Lee},
	title  = {Learning to Encode Text as Human-Readable Summaries using Generative Adversarial Networks},
	year   = {2018},
	url    = {https://openreview.net/forum?id=r1kNDlbCb},
}

@Article{Kodaira2018,
	author      = {Tomonori Kodaira and Mamoru Komachi},
	title       = {The Rule of Three: Abstractive Text Summarization in Three Bullet Points},
	abstract    = {Neural network-based approaches have become widespread for abstractive text summarization. Though previously proposed models for abstractive text summarization addressed the problem of repetition of the same contents in the summary, they did not explicitly consider its information structure. One of the reasons these previous models failed to account for information structure in the generated summary is that standard datasets include summaries of variable lengths, resulting in problems in analyzing information flow, specifically, the manner in which the first sentence is related to the following sentences. Therefore, we use a dataset containing summaries with only three bullet points, and propose a neural network-based abstractive summarization model that considers the information structures of the generated summaries. Our experimental results show that the information structure of a summary can be controlled, thus improving the performance of the overall summarization.},
	date        = {2018-09-28},
	eprint      = {http://arxiv.org/abs/1809.10867v1},
	eprintclass = {cs.CL},
	eprinttype  = {arXiv},
	file        = {:http\://arxiv.org/pdf/1809.10867v1:PDF},
	keywords    = {cs.CL},
}

@Article{Bacciu2018,
	author      = {Davide Bacciu and Antonio Bruno},
	title       = {Text Summarization as Tree Transduction by Top-Down TreeLSTM},
	abstract    = {Extractive compression is a challenging natural language processing problem. This work contributes by formulating neural extractive compression as a parse tree transduction problem, rather than a sequence transduction task. Motivated by this, we introduce a deep neural model for learning structure-to-substructure tree transductions by extending the standard Long Short-Term Memory, considering the parent-child relationships in the structural recursion. The proposed model can achieve state of the art performance on sentence compression benchmarks, both in terms of accuracy and compression rate.},
	date        = {2018-09-24},
	eprint      = {http://arxiv.org/abs/1809.09096v1},
	eprintclass = {cs.IR},
	eprinttype  = {arXiv},
	file        = {:http\://arxiv.org/pdf/1809.09096v1:PDF},
	keywords    = {cs.IR, cs.LG, cs.NE, stat.ML},
}
