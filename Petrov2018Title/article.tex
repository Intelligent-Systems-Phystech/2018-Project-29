\documentclass[12pt,twoside]{article}
\usepackage{jmlda}
\begin{document}
\Russian
%\NOREVIEWERNOTES
\title
    [Cross-Language Document Extractive Summarization with Neural Sequence Model] % TODO: название
    {Cross-Language Document Extractive Summarization with Neural Sequence Model}
\author
    [Захаров~П.\,С., Сельницкий~И.\,С., Кваша~П.\,А., Дьячков~Е.\,А, Петров~Е.\,Д.] % список авторов для колонтитула; не нужен, если основной список влезает в колонтитул
    {Захаров~П.\,С., Сельницкий~И.\,С., Кваша~П.\,А., Дьячков~Е.\,А, Петров~Е.\,Д.} % основной список авторов, выводимый в оглавление
    %[Захаров П.С., Сельницкий И.С., Кваша П.А., Дьячков Е.А, Петров Е.Д.] % список авторов, выводимый в заголовок; не нужен, если он не отличается от основного
\thanks
    {Научный руководитель:  Стрижов~В.\,В.
   Задачу поставил:  Хританков~А.\,С.
    Консультант:  Романов~А.\,В.}
\organization
    {Московский физико-технический институт}
\abstract
    { В статье исследуется модель, которая осуществляет сокращение исходного текста с сохранением начального смысла, кроме того это происходит на языке, который отличается от языка исходного документа. В статье происходит кртакое изложение текста, которое изменяет язык с помощью машинного перевода. Качество проделанной работы оценивается 2 характеристиками: сокращение исходного текста и осуществленный перевод укороченного текста. Работа данной модели осуществляется при помощи спроектированных нейронных сетей. Статья основана на базовой модели, которая учитывала малое количество набора данных, поэтому в данной статье мы планируем исследовать исходную базовую модель на более широкий спектр набора данных, который позволит обеспечить более корректную и улучшенную версию сущетвующей модели в будущем.
\bigskip

\textbf{Ключевые слова}: \emph {Сокращение текстов, машинный перевод, нейронные сети}.}

\maketitle

\section{Введение}
В настоящее время получили развитие различные базы данных, хранящие большое количетсво документов того или иного рода (научные или журналистские статьи, статистическая информация и т.д.). Эти документы требуется представлять в удобном для быстрой оценки виде, т.е. создать изложение текста. Большое количество данных привело к развитию машинного сокращения текстов. При решении задач подобного рода можно выделить два подхода: абстрактное и экстрактивное изложение. В первом случае сокращенный текст генерируется "с нуля", в то время как второй подход отбирает предложения из исходного текста. В данной работе рассматривается в основном экстрактивное сокращение, т.к. оно проще в реализации и в целом на настоящий момент показывает лучшие результаты . 

Необходимость делать сокращения текстов на других языках и развитие технологий машинного перевода подтолкнуло создание моделей, реализующих межъязыковое сокращение текстов (англ. Cross-Language Automatic Text Summarization). Наиболее простым решением проблемы является последовательное применение двух техник. Такие модели называются LateTrans и EarlyTrans - в первом случае сначала идет сокращение на языке оригинала, а затем перевод, во втором - наоборот. Обе концепции показали себя не лучшим образом в связи с несовершенством обеих технологий: неидеальный выход первой модели еще сильнее искажался второй. Wan и др.\cite{Wan:2010:CDS:1858681.1858775} предложили идею усовершенстованной LateTrans модели: при сокращении на языке оригинала учитывались не только информативность предложения, но и предсказание качества перевода. Помимо этого, Wan и др. \cite{Wan2018} реализовал систему, создающую изложения-кандидаты, полученные, разными способами, и отбирающую лучшие из них. Pontes и др.\cite{Pontes2018} использовали кластеризацию и сжатие исходных предложений для получения более информативных кандидатов.

Помимо совершенствования систем в целом, ведутся дальнейшие исследования в моноязыковом сокращении текстов, \cite{DBLP:journals/corr/WuSCLNMKCGMKSJL16}\cite{journals/corr/NallapatiZZ16}. Можно также отметить прогресс в, к примеру, задаче векторизации предложений \cite{conf/globalsip/ZhangSNPLSP17}. Модульная архитектура позволяет использовать эти наработки в содании более эффективных CLATS-моделей.

В данной работе предлагается развить идею Wan\cite{Wan:2010:CDS:1858681.1858775} в приложении к сокращению с переводом с английского языка на русский, используя более совершенные составляющие: в качестве базовой модели используется SummaRunner2016\cite{journals/corr/NallapatiZZ16}, для перевода - openNMT, описанная в \cite{journals/corr/BahdanauCB14}. Ставится задача решить проблемы модели сокращения текста, связанные с переносом на другую выбору документов, а также определить необходимость дополнительной предобработки текстов и границы применимости модели.

Для обучения SummaRunner используется исходная выборка - CNN/DailyMail corpus, а для обучения openNMT - параллельный корпус OPUS. Кроме того, имеются данные на русском языке для оценки ошибки. ...

\bibliographystyle{plain}
\bibliography{Petrov}

\end{document}
