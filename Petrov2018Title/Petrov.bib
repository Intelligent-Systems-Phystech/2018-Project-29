% Encoding: windows-1251

@InProceedings{DBLP:conf/pkdd/DlikmanL16,
  author    = {Alexander Dlikman and Mark Last},
  title     = {Using Machine Learning Methods and Linguistic Features in Single-Document Extractive Summarization},
  booktitle = {Proceedings of the Workshop on Interactions between Data Mining and Natural Language Processing, {DMNLP} 2016, co-located with the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases, {ECML-PKDD} 2016, Riva del Garda, Italy, September 23, 2016.},
  year      = {2016},
  pages     = {1--8},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/bib/conf/pkdd/DlikmanL16},
  crossref  = {DBLP:conf/pkdd/2016dmnlp},
  timestamp = {Tue, 02 Aug 2016 16:20:44 +0200},
  url       = {http://ceur-ws.org/Vol-1646/paper1.pdf},
}

@InProceedings{Ren:2017:LCS:3077136.3080792,
  author    = {Ren, Pengjie and Chen, Zhumin and Ren, Zhaochun and Wei, Furu and Ma, Jun and de Rijke, Maarten},
  title     = {Leveraging Contextual Sentence Relations for Extractive Summarization Using a Neural Attention Model},
  booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  year      = {2017},
  series    = {SIGIR '17},
  pages     = {95--104},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3080792},
  doi       = {10.1145/3077136.3080792},
  isbn      = {978-1-4503-5022-8},
  keywords  = {contextual sentence relation, extractive summarization, neural network},
  location  = {Shinjuku, Tokyo, Japan},
  numpages  = {10},
  url       = {http://doi.acm.org/10.1145/3077136.3080792},
}

@InProceedings{Wan:2011:UBI:2002472.2002659,
  author    = {Wan, Xiaojun},
  title     = {Using Bilingual Information for Cross-language Document Summarization},
  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1},
  year      = {2011},
  series    = {HLT '11},
  pages     = {1546--1555},
  address   = {Stroudsburg, PA, USA},
  publisher = {Association for Computational Linguistics},
  acmid     = {2002659},
  isbn      = {978-1-932432-87-9},
  location  = {Portland, Oregon},
  numpages  = {10},
  url       = {http://dl.acm.org/citation.cfm?id=2002472.2002659},
}

@InProceedings{P18-1061,
  author    = {Zhou, Qingyu and Yang, Nan and Wei, Furu and Huang, Shaohan and Zhou, Ming and Zhao, Tiejun},
  title     = {Neural Document Summarization by Jointly Learning to Score and Select Sentences},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  year      = {2018},
  pages     = {654--663},
  publisher = {Association for Computational Linguistics},
  location  = {Melbourne, Australia},
  url       = {http://aclweb.org/anthology/P18-1061},
}

@Article{journals/corr/NallapatiZZ16,
  title =	"SummaRuNNer: A Recurrent Neural Network based Sequence
		 Model for Extractive Summarization of Documents",
  author =	"Ramesh Nallapati and Feifei Zhai and Bowen Zhou",
  journal =	"CoRR",
  year = 	"2016",
  volume =	"abs/1611.04230",
  bibdate =	"2017-06-07",
  bibsource =	"DBLP,
		 http://dblp.uni-trier.de/db/journals/corr/corr1611.html#NallapatiZZ16",
  URL =  	"http://arxiv.org/abs/1611.04230",
}

@InProceedings{Wan:2010:CDS:1858681.1858775,
  author    = {Wan, Xiaojun and Li, Huiying and Xiao, Jianguo},
  title     = {Cross-language Document Summarization Based on Machine Translation Quality Prediction},
  booktitle = {Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics},
  year      = {2010},
  series    = {ACL '10},
  pages     = {917--926},
  address   = {Stroudsburg, PA, USA},
  publisher = {Association for Computational Linguistics},
  acmid     = {1858775},
  location  = {Uppsala, Sweden},
  numpages  = {10},
  url       = {http://dl.acm.org/citation.cfm?id=1858681.1858775},
}

@Article{DBLP:journals/corr/KleinKDSR17,
  author        = {Guillaume Klein and Yoon Kim and Yuntian Deng and Jean Senellart and Alexander M. Rush},
  title         = {OpenNMT: Open-Source Toolkit for Neural Machine Translation},
  journal       = {CoRR},
  year          = {2017},
  volume        = {abs/1701.02810},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/KleinKDSR17},
  eprint        = {1701.02810},
  timestamp     = {Mon, 13 Aug 2018 16:46:49 +0200},
  url           = {http://arxiv.org/abs/1701.02810},
}

@InProceedings{Jain2017a,
  author        = {Aditya Jain and Divij Bhatia and Manish K Thakur},
  title         = {Extractive Text Summarization Using Word Vector Embedding},
  booktitle     = {2017 International Conference on Machine Learning and Data Science ({MLDS})},
  year          = {2017},
  month         = {dec},
  publisher     = {{IEEE}},
  __markedentry = {[Silya:6]},
  doi           = {10.1109/mlds.2017.12},
}

@InProceedings{conf/globalsip/ZhangSNPLSP17,
  title =	"Semantic sentence embeddings for paraphrasing and text
		 summarization",
  author =	"Chi Zhang and Shagan Sah and Thang Nguyen and Dheeraj
		 Peri and Alexander Loui and Carl Salvaggio and Raymond
		 Ptucha",
  publisher =	"IEEE",
  year = 	"2017",
  bibdate =	"2018-03-13",
  bibsource =	"DBLP,
		 http://dblp.uni-trier.de/https://doi.org/10.1109/GlobalSIP.2017.8309051;
		 DBLP,
		 http://dblp.uni-trier.de/db/conf/globalsip/globalsip2017.html#ZhangSNPLSP17",
  booktitle =	"GlobalSIP",
  crossref =	"conf/globalsip/2017",
  ISBN = 	"978-1-5090-5990-4",
  pages =	"705--709",
  URL =  	"http://ieeexplore.ieee.org/xpl/mostRecentIssue.jsp?punumber=8303183",
}

@Article{Mehta2018,
  author      = {Parth Mehta and Prasenjit Majumder},
  title       = {Exploiting local and global performance of candidate systems for aggregation of summarization techniques},
  abstract    = {With an ever growing number of extractive summarization techniques being proposed, there is less clarity then ever about how good each system is compared to the rest. Several studies highlight the variance in performance of these systems with change in datasets or even across documents within the same corpus. An effective way to counter this variance and to make the systems more robust could be to use inputs from multiple systems when generating a summary. In the present work, we define a novel way of creating such ensemble by exploiting similarity between the content of candidate summaries to estimate their reliability. We define GlobalRank which captures the performance of a candidate system on an overall corpus and LocalRank which estimates its performance on a given document cluster. We then use these two scores to assign a weight to each individual systems, which is then used to generate the new aggregate ranking. Experiments on DUC2003 and DUC 2004 datasets show a significant improvement in terms of ROUGE score, over existing sate-of-art techniques.},
  date        = {2018-09-07},
  eprint      = {http://arxiv.org/abs/1809.02343v1},
  eprintclass = {cs.IR},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1809.02343v1:PDF},
  keywords    = {cs.IR, cs.AI, cs.CL},
}

@Article{Wan2018,
  author    = {Xiaojun Wan and Fuli Luo and Xue Sun and Songfang Huang and Jin-ge Yao},
  title     = {Cross-language document summarization via extraction and ranking of multiple summaries},
  journal   = {Knowledge and Information Systems},
  year      = {2018},
  month     = {jan},
  doi       = {10.1007/s10115-018-1152-7},
  publisher = {Springer Nature},
}

@InProceedings{Mihalcea_alanguage,
  author    = {Rada Mihalcea and Paul Tarau},
  title     = {A language independent algorithm for single and multiple document summarization},
  booktitle = {In Proceedings of IJCNLP’2005},
}

@Article{Zhang2018a,
  author      = {Xingxing Zhang and Mirella Lapata and Furu Wei and Ming Zhou},
  title       = {Neural Latent Extractive Document Summarization},
  abstract    = {Extractive summarization models require sentence-level labels, which are usually created heuristically (e.g., with rule-based methods) given that most summarization datasets only have document-summary pairs. Since these labels might be suboptimal, we propose a latent variable extractive model where sentences are viewed as latent variables and sentences with activated variables are used to infer gold summaries. During training the loss comes \emph{directly} from gold summaries. Experiments on the CNN/Dailymail dataset show that our model improves over a strong extractive baseline trained on heuristically approximated labels and also performs competitively to several recent models.},
  date        = {2018-08-22},
  eprint      = {http://arxiv.org/abs/1808.07187v2},
  eprintclass = {cs.CL},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1808.07187v2:PDF},
  keywords    = {cs.CL, cs.AI, cs.LG},
}

@InProceedings{N16-1012,
  author    = {Chopra, Sumit and Auli, Michael and Rush, Alexander M.},
  title     = {Abstractive Sentence Summarization with Attentive Recurrent Neural Networks},
  booktitle = {Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  year      = {2016},
  pages     = {93--98},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/N16-1012},
  location  = {San Diego, California},
  url       = {http://www.aclweb.org/anthology/N16-1012},
}

@Article{See2017,
  author      = {Abigail See and Peter J. Liu and Christopher D. Manning},
  title       = {Get To The Point: Summarization with Pointer-Generator Networks},
  abstract    = {Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.},
  date        = {2017-04-14},
  eprint      = {http://arxiv.org/abs/1704.04368v2},
  eprintclass = {cs.CL},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1704.04368v2:PDF},
  keywords    = {cs.CL},
}

@article{DBLP:journals/corr/WuSCLNMKCGMKSJL16,
  author    = {Yonghui Wu and
               Mike Schuster and
               Zhifeng Chen and
               Quoc V. Le and
               Mohammad Norouzi and
               Wolfgang Macherey and
               Maxim Krikun and
               Yuan Cao and
               Qin Gao and
               Klaus Macherey and
               Jeff Klingner and
               Apurva Shah and
               Melvin Johnson and
               Xiaobing Liu and
               Lukasz Kaiser and
               Stephan Gouws and
               Yoshikiyo Kato and
               Taku Kudo and
               Hideto Kazawa and
               Keith Stevens and
               George Kurian and
               Nishant Patil and
               Wei Wang and
               Cliff Young and
               Jason Smith and
               Jason Riesa and
               Alex Rudnick and
               Oriol Vinyals and
               Greg Corrado and
               Macduff Hughes and
               Jeffrey Dean},
  title     = {Google's Neural Machine Translation System: Bridging the Gap between
               Human and Machine Translation},
  journal   = {CoRR},
  volume    = {abs/1609.08144},
  year      = {2016},
  url       = {http://arxiv.org/abs/1609.08144},
  archivePrefix = {arXiv},
  eprint    = {1609.08144},
  timestamp = {Mon, 13 Aug 2018 16:46:24 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/WuSCLNMKCGMKSJL16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InCollection{Singh2017,
  author    = {Shashi Pal Singh and Ajai Kumar and Hemant Darbari and Anshika Rastogi and Shikha Jain and Nisheeth Joshi},
  title     = {Building Machine Learning System with Deep Neural Network for Text Processing},
  booktitle = {Information and Communication Technology for Intelligent Systems ({ICTIS} 2017) - Volume 2},
  publisher = {Springer International Publishing},
  year      = {2017},
  pages     = {497--504},
  month     = {aug},
  doi       = {10.1007/978-3-319-63645-0_56},
}

@InProceedings{Zhang2018b,
  author    = {Yong Zhang and Jinzhi Liao and Jiuyang Tang and Weidong Xiao and Yuheng Wang},
  title     = {Extractive Document Summarization Based on Hierarchical {GRU}},
  booktitle = {2018 International Conference on Robots {\&} Intelligent System ({ICRIS})},
  year      = {2018},
  month     = {may},
  publisher = {{IEEE}},
  doi       = {10.1109/icris.2018.00092},
}

@InCollection{Pontes2018,
  author    = {Elvys Linhares Pontes and St{\'{e}}phane Huet and Juan-Manuel Torres-Moreno and Andr{\'{e}}a Carneiro Linhares},
  title     = {Cross-Language Text Summarization Using Sentence and Multi-Sentence Compression},
  booktitle = {Natural Language Processing and Information Systems},
  publisher = {Springer International Publishing},
  year      = {2018},
  pages     = {467--479},
  doi       = {10.1007/978-3-319-91947-8_48},
}

@Misc{wang2018learning,
  author = {Yau-Shian Wang and Hung-Yi Lee},
  title  = {Learning to Encode Text as Human-Readable Summaries using Generative Adversarial Networks},
  year   = {2018},
  url    = {https://openreview.net/forum?id=r1kNDlbCb},
}

@Article{Kodaira2018,
  author      = {Tomonori Kodaira and Mamoru Komachi},
  title       = {The Rule of Three: Abstractive Text Summarization in Three Bullet Points},
  abstract    = {Neural network-based approaches have become widespread for abstractive text summarization. Though previously proposed models for abstractive text summarization addressed the problem of repetition of the same contents in the summary, they did not explicitly consider its information structure. One of the reasons these previous models failed to account for information structure in the generated summary is that standard datasets include summaries of variable lengths, resulting in problems in analyzing information flow, specifically, the manner in which the first sentence is related to the following sentences. Therefore, we use a dataset containing summaries with only three bullet points, and propose a neural network-based abstractive summarization model that considers the information structures of the generated summaries. Our experimental results show that the information structure of a summary can be controlled, thus improving the performance of the overall summarization.},
  date        = {2018-09-28},
  eprint      = {http://arxiv.org/abs/1809.10867v1},
  eprintclass = {cs.CL},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1809.10867v1:PDF},
  keywords    = {cs.CL},
}

@Article{journals/corr/BahdanauCB14,
  title =	"Neural Machine Translation by Jointly Learning to
		 Align and Translate",
  author =	"Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio",
  journal =	"CoRR",
  year = 	"2014",
  volume =	"abs/1409.0473",
  bibdate =	"2017-06-07",
  bibsource =	"DBLP,
		 http://dblp.uni-trier.de/db/journals/corr/corr1409.html#BahdanauCB14",
  URL =  	"http://arxiv.org/abs/1409.0473",
}

@Article{Bacciu2018,
  author      = {Davide Bacciu and Antonio Bruno},
  title       = {Text Summarization as Tree Transduction by Top-Down TreeLSTM},
  abstract    = {Extractive compression is a challenging natural language processing problem. This work contributes by formulating neural extractive compression as a parse tree transduction problem, rather than a sequence transduction task. Motivated by this, we introduce a deep neural model for learning structure-to-substructure tree transductions by extending the standard Long Short-Term Memory, considering the parent-child relationships in the structural recursion. The proposed model can achieve state of the art performance on sentence compression benchmarks, both in terms of accuracy and compression rate.},
  date        = {2018-09-24},
  eprint      = {http://arxiv.org/abs/1809.09096v1},
  eprintclass = {cs.IR},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1809.09096v1:PDF},
  keywords    = {cs.IR, cs.LG, cs.NE, stat.ML},
}

@Comment{jabref-meta: databaseType:bibtex;}
