\documentclass[12pt,twoside]{article}
\usepackage{jmlda}
\begin{document}
\Russian
%\NOREVIEWERNOTES
\title
    [Cross-Language Document Extractive Summarization with Neural Sequence Model] % TODO: название
    {Cross-Language Document Extractive Summarization with Neural Sequence Model}
\author
    [Захаров~П.\,С., Пискун~М.\,Г., Сельницкий~И.\,С., Кваша~П.\,А., Дьячков~Е.\,А, Петров~Е.\,Д.] % список авторов для колонтитула; не нужен, если основной список влезает в колонтитул
    {Захаров~П.\,С., Пискун~М.\,Г., Сельницкий~И.\,С., Кваша~П.\,А., Дьячков~Е.\,А, Петров~Е.\,Д.} % основной список авторов, выводимый в оглавление
    %[Захаров П.С, Пискун М.Г., Сельницкий И.С., Кваша П.А., Дьячков Е.А, Петров Е.Д.] % список авторов, выводимый в заголовок; не нужен, если он не отличается от основного
\thanks
    {Научный руководитель:  Стрижов~В.\,В.
   Задачу поставил:  Хританков~А.\,?.
    Консультант:  Романов~А.\,?.}
\organization
    {Московский физико-технический институт}
\abstract
    { В данной работе представлена модель образования краткого изложения текста на языке, отличном от текста документа. Для этого используется сокращение текста выбором предложений с последующим машинным переводом; при отборе предложений учитывается не только их содержание, но и оценка качества перевода. Исследуется зависимость качества сокращения от качества перевода. Перевод и сокращение осуществляются специально спроектированными для этих целей нейронными сетями. При этом базовая модель исследовалась на малом числе наборов данных; в этой работе идет дальнейшее рассмотрение переносимости этой модели на другие данные и внесение коррективов для улучшения модели в будущем.
\bigskip

\textbf{Ключевые слова}: \emph {Сокращение текстов, машинный перевод, нейронные сети}.}

\maketitle

\section{Введение}
В настоящее время получили развитие различные базы данных, хранящие большое количетсво документов того или иного рода (научные или журналистские статьи, статистическая информация и т.д.). Эти документы требуется представлять в удобном для быстрой оценки виде, т.е. создать изложение текста. Большое количество данных привело к развитию машинного сокращения текстов. При решении задач подобного рода можно выделить два подхода: абстрактное и экстрактивное изложение. В первом случае сокращенный текст генерируется "с нуля", в то время как второй подход отбирает предложения из исходного текста. В данной работе рассматривается в основном экстрактивное сокращение, т.к. оно проще в реализации и в целом на настоящий момент показывает лучшие результаты \cite{journals/corr/NallapatiZZ16}. 

Необходимость делать сокращения текстов на других языках и развитие технологий машинного перевода подтолкнуло создание моделей, реализующих межъязыковое сокращение текстов (англ. Cross-Language Automatic Text Summarization). Наиболее простым решением проблемы является последовательное применение двух техник. Такие модели называются LateTrans и EarlyTrans - в первом случае сначала идет сокращение на языке оригинала, а затем перевод, во втором - наоборот. Обе концепции показали себя не лучшим образом в связи с несовершенством обеих технологий: неидеальный выход первой модели еще сильнее искажался второй. Wan и др.\cite{Wan:2010:CDS:1858681.1858775} предложили идею усовершенстованной LateTrans модели: при сокращении на языке оригинала учитывались не только информативность предложения, но и предсказание качества перевода. Помимо этого, Wan и др. \cite{Wan2018} реализовал систему, создающую изложения-кандидаты, полученные, разными способами, и отбирающую лучшие из них. Pontes и др.\cite{Pontes2018} использовали кластеризацию и сжатие исходных предложений для получения более информативных кандидатов.

Помимо совершенствования систем в целом, ведутся дальнейшие исследования в моноязыковом сокращении текстов, \cite{Wu2016}\cite{journals/corr/NallapatiZZ16}. Можно также отметить прогресс в, к примеру, задаче векторизации предложений \cite{Zhang2018}. Модульная архитектура позволяет использовать эти наработки в содании более эффективных CLATS-моделей.

В данной работе предлагается развить идею Wu\cite{Wan:2010:CDS:1858681.1858775} в приложении к сокращению с переводом с английского языка на русский, используя более совершенные составляющие: в качестве базовой модели используется SummaRunner2016\cite{journals/corr/NallapatiZZ16}, для перевода - openNMT, описанная в \cite{journals/corr/BahdanauCB14}. Ставится задача решить проблемы модели сокращения текста, связанные с переносом на другую выбору документов, а также определить необходимость дополнительной предобработки текстов и границы применимости модели.

Для обучения SummaRunner используется исходная выборка - CNN/DailyMail corpus, а для обучения openNMT - параллельный корпус OPUS. Кроме того, имеются данные на русском языке для оценки ошибки. ...

\bibliographystyle{plain}
\bibliography{Zakharov2018Title}

\end{document}
