\documentclass[12pt,twoside]{article}
\usepackage{jmlda}
%\usepackage{notations}
\begin{document}
\Russian
%\NOREVIEWERNOTES
\title
    [Cross-Language Document Extractive Summarization with Neural Sequence Model] % TODO: название
    {Cross-Language Document Extractive Summarization with Neural Sequence Model}
\author
    [Захаров~П.\,С., Сельницкий~И.\,С., Кваша~П.\,А., Дьячков~Е.\,А, Петров~Е.\,Д.] % список авторов для колонтитула; не нужен, если основной список влезает в колонтитул
    {Захаров~П.\,С., Сельницкий~И.\,С., Кваша~П.\,А., Дьячков~Е.\,А, Петров~Е.\,Д.} % основной список авторов, выводимый в оглавление
    %[Захаров П.С, Сельницкий И.С., Кваша П.А., Дьячков Е.А, Петров Е.Д.] % список авторов, выводимый в заголовок; не нужен, если он не отличается от основного
\thanks
    {Научный руководитель:  Стрижов~В.\,В.
   Задачу поставил:  Хританков~А.\,С.
    Консультант:  Романов~А.\,В.}
\organization
    {Московский физико-технический институт}
\abstract
    { В данной работе представлена модель образования краткого изложения текста на языке, отличном от текста документа. Для этого используется сокращение текста выбором предложений с последующим машинным переводом; при отборе предложений учитывается не только их содержание, но и оценка качества перевода. Исследуется зависимость качества сокращения от качества перевода. Перевод и сокращение осуществляются специально спроектированными для этих целей нейронными сетями. При этом базовая модель исследовалась на малом числе наборов данных; в этой работе идет дальнейшее рассмотрение переносимости этой модели на другие данные и внесение коррективов для улучшения модели в будущем.
\bigskip

\textbf{Ключевые слова}: \emph {Аннотирование текстов, машинный перевод, нейронные сети}.}

\maketitle

\section{Введение}

Данное исследование посвящено задаче аннотирования, т.е. краткого изложения текстов. Задача машинного аннотирования возникла в связи с развитием крупных хранилищ документов (в данном исследовании - статей), которые требуется представить в удобном для быстрой оценки виде. При решении задач подобного рода можно выделить два подхода: абстрактное и экстрактивное изложение. В первом случае аннотация является полностью синтетической, в то время как второй подход отбирает предложения из исходного текста. В данной работе рассматривается в основном экстрактивное аннотирование, т.к. оно проще в реализации и в целом на настоящий момент показывает лучшие результаты \cite{journals/corr/NallapatiZZ16}. 

Необходимость делать сокращения текстов на других языках и развитие технологий машинного перевода подтолкнуло создание моделей, реализующих межъязыковое аннотирование текстов (англ. Cross-Language Automatic Text Summarization). Наиболее простым решением проблемы является последовательное применение двух техник. Такие модели называются LateTrans и EarlyTrans - в первом случае сначала идет изложение на языке оригинала, а затем перевод, во втором - наоборот. Обе концепции показали себя не лучшим образом в связи с несовершенством обеих технологий: неидеальный выход первой модели еще сильнее искажался второй. Wan и др.\cite{Wan:2010:CDS:1858681.1858775} предложили идею усовершенстованной LateTrans модели: при аннотировании на языке оригинала учитывались не только информативность предложения, но и предсказание качества перевода. Помимо этого, Wan и др. \cite{Wan2018} реализовал систему, создающую изложения-кандидаты, полученные, разными способами, и отбирающую лучшие из них. Pontes и др.\cite{Pontes2018} использовали кластеризацию и сжатие исходных предложений для получения более информативных предложений для отбора.

Помимо совершенствования систем в целом, ведутся дальнейшие исследования в моноязыковом аннотировании, \cite{journals/corr/NallapatiZZ16}\cite{DBLP:journals/corr/WuSCLNMKCGMKSJL16}. Можно также отметить прогресс в, к примеру, задаче векторизации предложений \cite{conf/globalsip/ZhangSNPLSP17}. Модульная архитектура позволяет использовать эти наработки в содании более эффективных CLATS-моделей.

В данной работе предлагается развить идею Wan \cite{Wan:2010:CDS:1858681.1858775} в приложении к сокращению с переводом с английского языка на русский, используя более совершенные составляющие: в качестве базовой модели используется SummaRunner2016 \cite{journals/corr/NallapatiZZ16}, для перевода - openNMT, описанная в \cite{journals/corr/BahdanauCB14}. Ставится задача решить проблемы модели сокращения текста, связанные с переносом на другую выбору документов, а также определить необходимость дополнительной предобработки текстов и границы применимости модели.

Для обучения SummaRunner используется исходная выборка - CNN/DailyMail corpus, а для обучения openNMT - параллельный корпус OPUS. Кроме того, имеются данные на русском языке для оценки качества итогового изложения.

\section{Постановка задачи}

В основе реализуемой модели лежит объединение модели моноязыкового аннотирования и модели предсказания качества машинного перевода. Ниже по отдельности указана постановка задачи для каждой модели, впоследствии описано итоговое решение.

\subsection{Аннотирование}

Для аннотирования используется трехслойная двухсторонняя рекуррентная нейронная сеть. Пусть $\mathbf{v}_{ij} \in \mathbf{V}_i, \mathbf{V}_i\in\mathfrak{D}$ - объекты (векторные представления слов в предложениях, предложения в документе), $\mathbf{y}_i\in \mathbf{Y}$ - бинарные целевые векторы. Слои первых двух слоев нейронной сети состоят из нейронов, которые описываются двумя \textit{гейтами} $\mathbf{u}_j$ и $\mathbf{r}_j$ по следующим формулам:
\begin{eqnarray}
\label{gates}
\mathbf{u_j} & = & \sigma\left( \mathbf{W}_{ux}\mathbf{x}_j+\mathbf{W}_{uh}\mathbf{h}_{j-1}+\mathbf{b}_j\right) \\
\mathbf{r}_j & = & \sigma\left( \mathbf{W}_{rx}\mathbf{x}_j+\mathbf{W}_{rh}\mathbf{h}_{j-1}+\mathbf{b}_r\right) \\
\mathbf{h}'_j & = & tanh\left( \mathbf{W}_{hx}\mathbf{x}_j+\mathbf{W}_{hh}\left( \mathbf{r}_j\odot \mathbf{h}_{j-1}\right) + \mathbf{b}_j\right) \\
\mathbf{h}_j & = & \left(1-\mathbf{u}_j\right)\odot \mathbf{h}'_j+\mathbf{u}_j\odot \mathbf{h}_{j-1}
\end{eqnarray} 

На первом слое строится две цепочки нейронов для каждого предложения в тексте. Для одной цепочки $\mathbf{x}_j=\mathbf{v}_{ij}, j\in \left\lbrace 1..N_i\right\rbrace$, для другой $\mathbf{x}_j=\mathbf{v}_{iN_i-j}, j\in \left\lbrace 1..N_i\right\rbrace$ - во второй цепочке слова подаются в обратном порядке. Эту цепочку будем называть обратной, а первую = прямой. Здесь $N_i$ - количетсво слов в i-ом предложении

На втором слое строятся такие же цепочки нейронов, для прямой: 
\begin{equation}
\label{forwardsent}
\mathbf{x}_j = \frac{1}{N_j}\sum\limits_{k=1}^{N_j}\left[\mathbf{h}_j^f, \mathbf{h}_j^b\right],
\end{equation} 
где $\mathbf{h}_j^f$ - скрытое состояние нейронов прямой цепочки для j-ого предложения, а $\mathbf{h}_j^b$ - обратной. Квадратные скобки означают конкатенацию векторов. Для обратной цепочки:
\begin{equation}
\label{backwardsent}
\mathbf{x}_j = \frac{1}{N_{M-j}}\sum\limits_{k=1}^{N_{M-j}}\left[\mathbf{h}_{M-j}^f, \mathbf{h}_{M-j}^b\right],
\end{equation} 
где $\mathbb{M}$ - число предложений в документе
Представление документа $\mathbf{d}$ формируется следующим образом:
\begin{equation}
\label{docrepr}
\mathbf{d}=tanh\left(W_d\frac{1}{M}\sum\limits_{j=1}^{M}\left[\mathbf{h}_j^f, \mathbf{h}_j^b\right]+\mathbf{b}_j\right),
\end{equation}
где $\mathbf{h}_j^f$ и $\mathbf{h}_j^b$ - скрытые состояния прямой и обратной цепочек на втором слое.

Для классификации используется логистический слой:
\begin{equation}
\label{logreg}
P\left(y_j=1|\mathbf{h}_j, \mathbf{s}_j, \mathbf{d}\right)=\sigma\left(\mathbf{W}_c\mathbf{h}_j+\mathbf{h}^T_j\mathbf{W}_s\mathbf{d}-\mathbf{h}_j^T\mathbf{W}_rtanh\left(\mathbf{s}_j\right)+\mathbf{W}_{ap}\mathbf{p}_j^a+\mathbf{W}_{rp}\mathbf{p}_j^r+\mathbf{b}\right)
\end{equation}
Здесь $\mathbf{s}_j$ - динамическое представление аннотации на j-ом шаге, а $\mathbf{p}_j^a$ и $\mathbf{p}_j^r$ - абсолютные и относительные положения в документе. Члены, обозначенные $\mathbf{W}$ и $\mathbf{b}$ с индексами, являются праметрами модели. Представление аннотации определяется следующим образом:
\begin{equation}
\label{sumrepr}
\mathbf{s}_j=\sum\limits_{i=1}^{j-1}\mathbf{h}_iP\left(y_i=1|\mathbf{h}_j, \mathbf{s}_j, \mathbf{d}\right)
\end{equation}
Ставится задача минимизовать логистическую функцию правдоподобия:
\begin{equation}
\label{loglikelihood}
l\left(\mathbf{W}_\alpha, \mathbf{b}_\beta\right) = -\sum\limits_{k=1}^{D}\sum\limits_{j=1}^{M_k}\left(y_j^klogP\left(y_j^k=1|\mathbf{h}_j^k, \mathbf{s}_j^k, \mathbf{d}_k\right)+\left(1-y_j^k\right)log\left(1-P\left(y_j^k=1|\mathbf{h}_j^k, \mathbf{s}_j^k, \mathbf{d}_k\right)\right)
\end{equation}
Полученное мягкое предсказание в дальнейшем используется для формирования конечного прогноза
\subsection{Предсказание качества машинного перевода}
Для предсказания качества машинного перевода используется $\epsilon$-SVR метод. Пусть $\mathfrak{D}=\left\lbrace x_i\right\rbrace$ - объекты (векторные представления предложений), $\mathbf{Y}=\left\lbrace y_i\right\rbrace$ - целевые векторы. Требуется найти гладкую функцию $f$ такую, что:
\begin{equation}
\label{esvr}
\underset{w, b, \xi, \xi^*}{min} \frac{1}{2}w^Tw+C\sum\limits_{i=1}^n\xi_i+C\sum\limits_{i=1}^n\xi_i^*
\end{equation}
с ограничениями
\begin{eqnarray}
wwww..
\end{eqnarray}
\bibliographystyle{ieeetr}
\bibliography{Zakharov2018Title}

\end{document}
