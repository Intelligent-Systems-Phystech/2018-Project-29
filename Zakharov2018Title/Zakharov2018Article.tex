\documentclass[12pt,twoside]{article}
\usepackage{jmlda}
%\usepackage{notations}
\begin{document}
\Russian
%\NOREVIEWERNOTES
\title
    [Cross-Language Document Extractive Summarization with Neural Sequence Model] % TODO: название
    {Cross-Language Document Extractive Summarization with Neural Sequence Model}
\author
    [Захаров~П.\,С., Сельницкий~И.\,С., Кваша~П.\,А., Дьячков~Е.\,А, Петров~Е.\,Д.] % список авторов для колонтитула; не нужен, если основной список влезает в колонтитул
    {Захаров~П.\,С., Сельницкий~И.\,С., Кваша~П.\,А., Дьячков~Е.\,А, Петров~Е.\,Д.} % основной список авторов, выводимый в оглавление
    %[Захаров П.С, Сельницкий И.С., Кваша П.А., Дьячков Е.А, Петров Е.Д.] % список авторов, выводимый в заголовок; не нужен, если он не отличается от основного
\thanks
    {Научный руководитель:  Стрижов~В.\,В.
   Задачу поставил:  Хританков~А.\,С.
    Консультант:  Романов~А.\,В.}
\organization
    {Московский физико-технический институт}
\abstract
    { В данной работе представлена модель реферирования текстов на языке, отличном от текста документа. Для этого используется сокращение текста выбором предложений с последующим машинным переводом; при отборе предложений учитывается не только их содержание, но и оценка качества перевода. Исследуется зависимость качества сокращения от качества перевода. Перевод и сокращение осуществляются специально спроектированными для этих целей нейронными сетями. При этом базовая модель исследовалась на малом числе наборов данных; в этой работе идет дальнейшее рассмотрение переносимости этой модели на другие данные и внесение коррективов для улучшения модели в будущем.
\bigskip

\textbf{Ключевые слова}: \emph {Аннотирование текстов, машинный перевод, нейронные сети}.}

\maketitle

\section{Введение}

Данное исследование посвящено задаче реферирования, т.е. краткого изложения текстов. Задача машинного реферирования возникла в связи с развитием крупных хранилищ документов (в данном исследовании - статей), которые требуется представить в удобном для быстрой оценки виде. При решении задач подобного рода можно выделить два подхода: обобщение и извлечение. В первом случае сокращенный текст генерируется синтетически, в то время как второй подход отбирает предложения из исходного текста. В данной работе рассматривается в основном извлечение, т.к. оно проще в реализации и на настоящий момент показывает лучшие результаты \cite{journals/corr/NallapatiZZ16}. 

Необходимость делать сокращения текстов на других языках и развитие технологий машинного перевода подтолкнуло создание моделей, реализующих межъязыковое реферирование текстов (англ. Cross-Language Automatic Text Summarization). Наиболее простым решением проблемы является последовательное применение двух инструментов - моноязыкового реферирования и машинного перевода. Такие модели бывают двух типов - LateTrans и EarlyTrans - в первом случае сначала идет сокращение на языке оригинала, а затем перевод, во втором - наоборот. Обе концепции показали себя не лучшим образом в связи с несовершенством обеих технологий: неидеальный выход первой модели еще сильнее искажался второй. Wan и др.\cite{Wan:2010:CDS:1858681.1858775} предложили идею усовершенстованной LateTrans модели: при реферировании на языке оригинала учитывались не только информативность предложения, но и предсказание качества перевода. Помимо этого, Wan и др. \cite{Wan2018} реализовал систему, создающую изложения-кандидаты, полученные, разными способами, и отбирающую лучшие из них. Pontes и др.\cite{Pontes2018} использовали кластеризацию и сжатие исходных предложений для получения более информативных предложений для отбора.

Помимо совершенствования систем в целом, ведутся дальнейшие исследования в моноязыковом реферировании, \cite{journals/corr/NallapatiZZ16}\cite{DBLP:journals/corr/WuSCLNMKCGMKSJL16}. Можно также отметить работы по векторизации предложений \cite{conf/globalsip/ZhangSNPLSP17}. Модульная архитектура позволяет использовать эти наработки в содании более эффективных CLATS-моделей.

В данной работе предлагается развить идею Wan \cite{Wan:2010:CDS:1858681.1858775} в приложении к сокращению с переводом с английского языка на русский. Данная архитектура предполагает предсказание качества машинного перевода для исходных предложений и учет этих предсказаний при отборе кандидатов из предложений. После совершенного с помощью SummaRunner2016 \cite{journals/corr/NallapatiZZ16} извлечения предложений на английском языке полученные сокращенные тексты переводятся. В базовой модели для перевода используется пакет openNMT, описанный в \cite{journals/corr/BahdanauCB14}. Для обучения нейронной сети отбора предложений используется logloss \textbf{(термин на русском?)}, для обучения модели предсказания - MSE и коэффициент Пирсона, для конечной оценки качества - ROUGE. Ставится задача решить проблемы модели сокращения текста, связанные с переносом на другую выборку документов, а также определить необходимость дополнительной предобработки текстов и границы применимости модели.

Для обучения SummaRunner используется исходная выборка - CNN/DailyMail corpus, а для обучения openNMT - параллельный корпус OPUS. Кроме того, имеются данные на русском языке для оценки качества итогового изложения. \textbf{будет уточнено позднее, когда появится более подробная информация}

\section{Постановка задачи}

В основе реализуемой модели лежит объединение модели моноязыкового аннотирования и модели предсказания качества машинного перевода. В следующих 2 подразделах по отдельности поставлены задачи для каждой из двух моделей, в третьем описано их объединение.

\subsection{Извлечение}

Для реферирования используется трехслойная двухсторонняя рекуррентная нейронная сеть. Пусть $\mathfrac{D} = \left(\mathfrak{V}, \mathbf{Y}\right)$ - выборка (предложения в документе и бинарный целевой вектор), где $\mathfrak{V} = \left\lbrace\mathbf{V}_i\right\rbrace, \mathbf{V}_i \in \mathbb{R}^{N_i\times n}$ - набор предложений, $i \in \left\lbrace 1,. M\right\rbrace$. При этом $\mathbf{V}_i = \left[\mathbf{v}_{ij}\right] \in \mathbb{R}^{N_i\times n}$ - предложения, состоящие из векторных представлений слов длиной $n$. Слои первых двух слоев нейронной сети состоят из нейронов, которые описываются двумя \textit{гейтами} $\mathbf{u}_j$ и $\mathbf{r}_j$ по следующим формулам:
\begin{eqnarray}
\label{gates}
\mathbf{u_j} & = & \sigma\left( \mathbf{W}_{ux}\mathbf{x}_j+\mathbf{W}_{uh}\mathbf{h}_{j-1}+\mathbf{b}_j\right) \\
\mathbf{r}_j & = & \sigma\left( \mathbf{W}_{rx}\mathbf{x}_j+\mathbf{W}_{rh}\mathbf{h}_{j-1}+\mathbf{b}_r\right) \\
\mathbf{h}'_j & = & tanh\left( \mathbf{W}_{hx}\mathbf{x}_j+\mathbf{W}_{hh}\left( \mathbf{r}_j\odot \mathbf{h}_{j-1}\right) + \mathbf{b}_j\right) \\
\mathbf{h}_j & = & \left(1-\mathbf{u}_j\right)\odot \mathbf{h}'_j+\mathbf{u}_j\odot \mathbf{h}_{j-1}
\end{eqnarray} 

На первом слое строится две цепочки нейронов для каждого предложения в тексте. Для одной цепочки $\mathbf{x}_j=\mathbf{v}_{ij}, j\in \left\lbrace 1..N_i\right\rbrace$, для другой $\mathbf{x}_j=\mathbf{v}_{iN_i-j}, j\in \left\lbrace 1..N_i\right\rbrace$ - во второй цепочке слова подаются в обратном порядке. Эту цепочку будем называть обратной, а первую = прямой. Здесь $N_i$ - количетсво слов в i-ом предложении

На втором слое строятся такие же цепочки нейронов, для прямой: 
\begin{equation}
\label{forwardsent}
\mathbf{x}_j = \frac{1}{N_j}\sum\limits_{k=1}^{N_j}\left[\mathbf{h}_j^f, \mathbf{h}_j^b\right],
\end{equation} 
где $\mathbf{h}_j^f$ - скрытое состояние нейронов прямой цепочки для j-ого предложения, а $\mathbf{h}_j^b$ - обратной. Квадратные скобки означают конкатенацию векторов. Для обратной цепочки:
\begin{equation}
\label{backwardsent}
\mathbf{x}_j = \frac{1}{N_{M-j}}\sum\limits_{k=1}^{N_{M-j}}\left[\mathbf{h}_{M-j}^f, \mathbf{h}_{M-j}^b\right],
\end{equation} 
где $M$ - число предложений в документе
Представление документа $\mathbf{d}$ формируется следующим образом:
\begin{equation}
\label{docrepr}
\mathbf{d}=tanh\left(W_d\frac{1}{M}\sum\limits_{j=1}^{M}\left[\mathbf{h}_j^f, \mathbf{h}_j^b\right]+\mathbf{b}_j\right),
\end{equation}
где $\mathbf{h}_j^f$ и $\mathbf{h}_j^b$ - скрытые состояния прямой и обратной цепочек на втором слое.

Для классификации используется логистический слой:
\begin{equation}
\label{logreg}
P\left(y_j=1|\mathbf{h}_j, \mathbf{s}_j, \mathbf{d}\right)=\sigma\left(\mathbf{W}_c\mathbf{h}_j+\mathbf{h}^T_j\mathbf{W}_s\mathbf{d}-\mathbf{h}_j^T\mathbf{W}_rtanh\left(\mathbf{s}_j\right)+\mathbf{W}_{ap}\mathbf{p}_j^a+\mathbf{W}_{rp}\mathbf{p}_j^r+\mathbf{b}\right)
\end{equation}
Здесь $\mathbf{s}_j$ - динамическое представление аннотации на j-ом шаге, а $\mathbf{p}_j^a$ и $\mathbf{p}_j^r$ - абсолютные и относительные положения в документе. Члены, обозначенные $\mathbf{W}$ и $\mathbf{b}$ с индексами, являются праметрами модели. Представление аннотации определяется следующим образом:
\begin{equation}
\label{sumrepr}
\mathbf{s}_j=\sum\limits_{i=1}^{j-1}\mathbf{h}_iP\left(y_i=1|\mathbf{h}_j, \mathbf{s}_j, \mathbf{d}\right)
\end{equation}

Ставится задача минимизовать логистическую функцию правдоподобия:
\begin{equation}
\begin{gathered}
\label{loglikelihood}
l\left(\mathbf{W}, \mathbf{b}\right) = -\sum\limits_{k=1}^{D}\sum\limits_{j=1}^{M_k}\left(y_j^klogP\left(y_j^k=1|\mathbf{h}_j^k, \mathbf{s}_j^k, \mathbf{d}_k\right)+ \\ 
+ \left(1-y_j^k\right)log\left(1-P\left(y_j^k=1|\mathbf{h}_j^k, \mathbf{s}_j^k, \mathbf{d}_k\right)\right) \rightarrow min
\end{gathered}
\end{equation}
Полученное мягкое предсказание в дальнейшем используется для формирования конечного прогноза.
\subsection{Предсказание качества машинного перевода}

Пусть $\mathfrac{D} = \left(\mathfrak{V}, \mathbf{Y}\right) \subset \mathbb{R}^{m\times M}\times\mathbb{R}^M$ - выборка (объекты и целевой вектор), $\mathfrak{V}=\left[\mathbf{V}_i\right] \in \mathbb{R}^m$ - объекты (предложения). Подчеркнем, что ввиду решения другой задачи в этом подразделе представление предложений отличается - здесь они сами являются объектами, в то время как в предыдущем объектами были слова предложений. 

Для предсказания качества машинного перевода используется $\epsilon$-SVR метод. Требуется найти гладкую функцию $\mathbf{f}$ такую, что:
\begin{equation}
\label{esvr}
\underset{\mathbf{w}, b, \mathbf{\xi}, \mathbf{\xi^*}}{min} \frac{1}{2}\mathbf{w}^T\mathbf{w}+C\sum\limits_{i=1}^n\xi_i+C\sum\limits_{i=1}^n\xi_i^*
\end{equation}
при условии, что
\begin{eqnarray}
\mathbf{w}^T\mathbf{f}(\mathbf{V}_i)+b-y_i & \leq & \epsilon+\xi_i \nonumber \\
y_i - \mathbf{w}^T\mathbf{f}(\mathbf{V}_i) - b & \leq & \epsilon+\xi^*_i \nonumber \\
\epsilon, \xi_i, \xi_i^* \geq 0, i=1,..,M
\end{eqnarray}

Метриками качества являются
\begin{eqnarray}
MSE & = &\frac{1}{M}\sum\limits_{i=1}^{M}\left(\hat{y_i}-y_i\right)^2\quad \nonumber \text{-- среднеквадратичная ошибка и}  \\
\rho & = & \frac{\sum\limits_{i=1}^{M}\left(y_i-\overline{y}\right)\left(\hat{y_i}-\overline{\hat{y}}\right)}{Ms_ys_{\hat{y}}}\quad \nonumber \text{ -- коэффициент Пирсона} 
\end{eqnarray}
\bibliographystyle{ieeetr}
\bibliography{Zakharov2018Title}

\end{document}
