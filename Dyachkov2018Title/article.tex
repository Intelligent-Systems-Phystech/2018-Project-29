\documentclass[12pt,twoside]{article}
\usepackage{jmlda}

%\NOREVIEWERNOTES
\title
    [Cross-Language Document Extractive Summarization with Neural Sequence Model] % TODO: название
    {Cross-Language Document Extractive Summarization with Neural Sequence Model}
\author
    [Захаров П.С., Сельницкий И.С., Кваша П.А., Дьячков Е.А, Петров Е.Д.] % список авторов для колонтитула; не нужен, если основной список влезает в колонтитул
    {Захаров П.С., Сельницкий И.С., Кваша П.А., Дьячков Е.А, Петров Е.Д.} % основной список авторов, выводимый в оглавление
    [Захаров П.С., Сельницкий И.С., Кваша П.А., Дьячков Е.А, Петров Е.Д.] % список авторов, выводимый в заголовок; не нужен, если он не отличается от основного
\thanks
    {Научный руководитель:  Стрижов~В.\,В.
   Задачу поставил:  Хританков~А.\,С.
    Консультант:  Романов~А.\,В.}
\organization
    {Московский физико-технический институт}
\abstract
    { В данной работе исследуется зависимость качества сокращения текста от качества перевода. Текст представляет собой краткое содержание документа на языке, отличном от языка  написания документа. Для его получения  используется сокращение документа выбором предложений с последующим машинным переводом; при отборе предложений учитывается не только их содержание, но и оценка качества перевода. Также в работе исследуются условия, при которых возможен перенос обучения на набор данных на другом языке. 
\bigskip

\textbf{Ключевые слова}: \emph {Аннотирование текстов, машинный перевод, нейронные сети}.}

\begin{document}
\maketitle
\section{1. Введение}
 Межъязыковое реферирование текстов (англ. Cross-Language Automatic Text Summarization) опредляется, как задача составления краткого изложения текста на языке, отличном от языка текста документа(Wan et al., 2010). В наше время огромный объем информации доступен в Интернете, однако большинство людей не может с ним ознакомится из-за того, что информация представлена на незнакомом читателю языке. 
 
Необходимость делать сокращение текстов на других языках и развитие технологий машинного перевода подтолкнуло к созданию моделей, реализующщих межъязыковое реферирование текстов. Наиболее  простыми решениями являются EarlyTrans и LateTrans, суть которых заключается в последовательном применении двух инструментов -  машинного перевода и моноязыкового реферирования,  однако они отличаются поледовательностью применения этих инструментов: первая -  сначала переводит  текст на другой язык а потом производит сокращение,  вторая - наоборот. Обе концепции были слишком неточны, так как неточный результат первой модели еще сильнее искажался второй. Позже Wan \cite{Wan:2010:CDS:1858681.1858775} усоверешнствовал вторую модель, став учитывать не только инфрмативность, но и качество перевода, также Wan \cite{Wan:2011:UBI:2002472.2002659} предложил одновременно ранжировать предложения на исходном языке и на языке, на который надо перевести. Linhares Pontes\cite{Pontes2018} и его группа предложили кроме того, чтобы использовать инофрмацию с двух языков, еще и сжимать исходные предложения.

В этом исследовании мы фокусируемся на сокращении английского текста с последующим переводом на русский язык.
Входными параметроми являются наборы текстов на английсклм языке а выходными - краткое изложение этих документов на русском языке. Чтобы получить краткое изложение на русском языке необходимо произвести машинный перевод с английского на русский. Однако качество машинного перевода далеко не идеально: результаты перевода содержат множество ошибок и шумов. Это затрудняет задачу краткого изложения текста на разных языках, поскольку ошибки и шумы, вызванные машинным переводом, оказывают большое негативное влияние на оценку предложения или краткое изложение на целевом языке. В работе предлагается развить идею Wan \cite{Wan:2010:CDS:1858681.1858775}, предполагающую предсказание качества машинного паеревода для исходных предложений с целью дальнейшего отбора предложений, в приложении к сокращению с переводом с английского языка на русский. Планирутся использовать имеющуюся модель сокращения текстов SummaRuNNer, предложенную  Nallapati \cite{DBLP:journals/corr/NallapatiZZ16}, стоит отметь существующее улучшение этой модели \cite{Ren:2017:LCS:3077136.3080792}, в котором оценивалось  связанность пердложения с  предыдущими и следующими за ним приидложениями. Для обучения нейросети будет использоваться минимизация логистической функции потерь. Для перовода воспользуемся фреймворком openNMT, описанным в \cite{DBLP:journals/corr/KleinKDSR17}, для обучения которой используется минимизация среднеквадратичной ошибки и коэффициента Пирсона. Для конечной оценки качества используется метрика ROUGE. Ставится задача решить проблемы модели сокращения текста, связанные с переносом на другую выборку документов, а также определить необходимость дополнительной предобработки текстов и границы применимости модели.

Для обучения SummaRunner используется исходный датасет - CNN/DailyMail corpus, а для обучения openNMT - параллельный корпус OPUS.

\section{2. Постановка задачи}
В основе реализуемой модели лежит объединение модели моноязыкового аннотирования и модели предсказания качества машинного перевода. В следующих двух подразделах по отдельности поставлены задачи для каждой из двух моделей, в третьем описано их объединение.

\subsection{2.1 Извлечение предложений} \label{subsec:summ}
В этой работе мы рассматриваем извлечение предложений  как проблему классификации последовательностей, в которой для каждого предложение принимается решение включать его в вывод модели, учитывая уже выбранные предложения.  Используется рекуррентная нейронная сеть  в качестве основоного блока классификатора последовательности. Введем следующие обозначения: $\mathfrak{D} = \left(\mathbf{X}, \mathbf{y}\right)$ - выборка (предложение в документе и бинарное целевое значение для этого предложения), где $\mathbf{X} = \left\lbrace\mathbf{X}_i\right\rbrace, \mathbf{X}_i \in \mathbb{R}^{N_i\times n}$ - набор предложений, $i \in \left\lbrace 1,. M\right\rbrace$. При этом $\mathbf{X}_i = \left[\mathbf{x}_{ij}\right] \in \mathbb{R}^{N_i\times n}$ - предложения, состоящие из векторных представлений слов длиной $n$.
Нейронная сеть задается двумя  гейтами  $\mathbf{u}_j$ и $\mathbf{r}_j$  следующими формулами:
\begin{eqnarray}
\label{gates}
\mathbf{u_j} & = & \sigma\left( \mathbf{W}_{ux}\mathbf{x}_j+\mathbf{W}_{uh}\mathbf{h}_{j-1}+\mathbf{b}_j\right)  \\
\mathbf{r}_j & = & \sigma\left( \mathbf{W}_{rx}\mathbf{x}_j+\mathbf{W}_{rh}\mathbf{h}_{j-1}+\mathbf{b}_r\right) \\
\mathbf{h}'_j & = & tanh\left( \mathbf{W}_{hx}\mathbf{x}_j+\mathbf{W}_{hh}\left( \mathbf{r}_j\odot \mathbf{h}_{j-1}\right) + \mathbf{b}_j\right)  \\
\mathbf{h}_j & = & \left(1-\mathbf{u}_j\right)\odot \mathbf{h}'_j+\mathbf{u}_j\odot \mathbf{h}_{j-1} 
\end{eqnarray}
где \textbf{W} и \textbf{b} глобальные параметры рекурентной нейронной сети, $\mathbf{h}_j$ вещественный вектор скрытого сотсояния в момент времени j, $ \mathbf{x}_j$ 
соответсвенно входной вектор,  $\odot$ - произведение Адамара.

Модель состоит из двухслойной двунаправленной рекурентной нейронной сети (RNN). Первый уровень RNN работает на уровне слова - $\mathbf{x}_j=\mathbf{x}_{ij}, j\in \left\lbrace 1..N_i\right\rbrace$ и последовательно вычисляет представления скрытого слоя в каждой позиции слова на основе текущего вхождения и предыдущего  состояния скрытого слоя. Мы также используем другую RNN на уровне слов - $\mathbf{x}_j=\mathbf{x}_{iN_i-j}, j\in \left\lbrace 1..N_i\right\rbrace$  , которая идет назад от последнего слова к первому, и мы относим пару прямых и обратных RNN как двунаправленную RNN.

Модель также состоит из второго слоя двунаправленной RNN, которая работает на уровне предложения и принимает в качестве входных данных  объединенные скрытые состояния двунаправленной RNN уровня слова. Для прямой RNN:
\begin{equation}
\label{forwardsentence}
\mathbf{x}_j = \frac{1}{N_j}\sum\limits_{k=1}^{N_j}\left[\mathbf{h}_k^f, \mathbf{h}_k^b\right],
\end{equation} 
 Для обратной:
\begin{equation}
\label{backwardsentence}
\mathbf{x}_j = \frac{1}{N_{M-j}}\sum\limits_{k=1}^{N_{M-j}}\left[\mathbf{h}_{M-k}^f, \mathbf{h}_{M-k}^b\right],
\end{equation} 
где $\mathbf{h}_j=k^f$ и$\mathbf{h}_j=k^b$- скрытое состояние нейронов относящееся к j-ому преложению прямой и обратной RNN уровня слов.  $ \mathbf{N}_j$ - количество слов в предложении. Квадратные скобки означают конкатенацию векторов. 

Представление документа $d$ формируется следующим образом:
\begin{equation}
\label{docrepresent}
\mathbf{d}=tanh\left(W_d\frac{1}{M}\sum\limits_{j=1}^{M}\left[\mathbf{h}_j^f, \mathbf{h}_j^b\right]+\mathbf{b}_j\right),
\end{equation}
где M количество предложений а  $\mathbf{h}_j^f$ и $\mathbf{h}_j^b$ - скрытые состояния прямой и обратной цепочек RNN на уровне предложений

Для классификации каждое предложение подается на вход логистическому слою, который принимает решение, будет ли предложение принадлежать аннотации:
\begin{equation}
\label{logreg}
P\left(y_j=1|\mathbf{h}_j, \mathbf{s}_j, \mathbf{d}\right)=\sigma\left(\mathbf{W}_c\mathbf{h}_j+\mathbf{h}^T_j\mathbf{W}_s\mathbf{d}-\mathbf{h}_j^T\mathbf{W}_rtanh\left(\mathbf{s}_j\right)+\mathbf{W}_{ap}\mathbf{p}_j^a+\mathbf{W}_{rp}\mathbf{p}_j^r+\mathbf{b}\right)
\end{equation}
где  $ \mathbf{y}_j$  - бинарное значение, показывающее, будет ли j-ое предложение принадлежать аннотации, $\mathbf{s}_j$ - динамическое представление аннотации на j-ом шаге, $\mathbf{p}_j^a$ и $\mathbf{p}_j^r$ - абсолютные и относительные положения в документе, $ \mathbf{h}_j$ скрытое сотсояние второй нейронной сети, работающей на уровне предложений,  $(\mathbf{W}_c\mathbf{h}$  отвечает за информативность j-ого предложения, $\mathbf{h}^T_j\mathbf{W}_s\mathbf{d}$ отвечает за значимость предложения по отношению к документу,  $\mathbf{h}_j^T\mathbf{W}_rtanh\left(\mathbf{s}\right)$ фиксирует избыточность предложения относительно текущего состояния аннотации, $\mathbf{W}_{ap}$  и $\mathbf{W}_{rp}$ коэффициенты важности абосютного и относительного положения предложения в документе.

Ставится задача минимизовать логистическую функцию правдоподобия:
\begin{equation}
\begin{gathered}
\label{loglikelihood}
l\left(\mathbf{W}, \mathbf{b}\right) = -\sum\limits_{k=1}^{D}\sum\limits_{j=1}^{M_k}\left(y_j^klogP\left(y_j^k=1|\mathbf{h}_j^k, \mathbf{s}_j^k, \mathbf{d}_k\right)\right)+ \\ 
+ \left(1-y_j^k\right)log\left(1-P\left(y_j^k=1|\mathbf{h}_j^k, \mathbf{s}_j^k, \mathbf{d}_k\right)\right) \rightarrow min
\end{gathered}
\end{equation}
Полученное мягкое предсказание в дальнейшем используется для формирования конечного прогноза.


\subsection{2.2 Предсказание качества машинного перевода}  \label{MT}
Пусть $\mathfrak{D} = \left(\mathbf{X}, \mathbf{y}\right) \subset \mathbb{R}^{m\times M}\times\mathbb{R}^M$ - выборка (объекты и целевой вектор), $\mathbf{X}=\left[\mathbf{x}_i\right] \in \mathbb{R}^m$ - объекты (предложения). В этом разделе мы будем пользоваться $\mathbf{\epsilon}$-SVR методом (Vapnik 1995) для задачи прогнозирования качества машинного перевода предложения. Требуется найти такую гладкую функцию $f$, которая аппроксиммирует отношение между точками $(\mathbf{x}_i, y_i )$:

\begin{equation}
\label{esvr}
\underset{\mathbf{w}, b, \mathbf{\xi}, \mathbf{\xi^*}}{min} \frac{1}{2}\mathbf{w}^T\mathbf{w}+C\sum\limits_{i=1}^n\xi_i+C\sum\limits_{i=1}^n\xi_i^*
\end{equation}
при условии, что
\begin{eqnarray}
\mathbf{w}^T\mathbf{f}(\mathbf{V}_i)+b-y_i & \leq & \epsilon+\xi_i \nonumber \\
y_i - \mathbf{w}^T\mathbf{f}(\mathbf{V}_i) - b & \leq & \epsilon+\xi^*_i \\
\epsilon, \xi_i, \xi_i^* \geq 0, i=1,..,M \nonumber
\end{eqnarray}

Константа C>0 является параметром, определяющий компромисс между  гладкостью f и величины, до которой допускаются отклонения, превышающие  $\mathbf{\epsilon}$.

Для оценки результатов предсказания используются две метрики
\begin{eqnarray}
  \mathbf{MSE} & = &\frac{1}{M}\sum\limits_{i=1}^{M}\left(\hat{y_i}-y_i\right)^2\quad \nonumber \text{- среднеквадратичная ошибка}  \\
 \mathbf{\rho} & = & \frac{\sum\limits_{i=1}^{M}\left(y_i-\overline{y}\right)\left(\hat{y_i}-\overline{\hat{y}}\right)}{Ms_ys_{\hat{y}}}\quad \nonumber \text{ - коэффициент Пирсона,}
\end{eqnarray} 

где $\overline{y}$ и $\overline{\hat{y}}$ - средние значения  точного результата и предсказанных предложений  соотвественно , а  $s_y$ и $s_{\hat{y}}$ - их среднеквадратичные отклонения.

После обучения выданные оценки качества нормируются максимальным значением в документе: $\tilde{y_i} = \frac{\hat{y_i}}{\underset{i}{max}\hat{y_i}}$

\subsection{2.3 Предсказание качества машинного перевода}
После получения оценки информативности каждого предложения $y_{INF,i}$ и  качества перевода  {$y_{MT,i}$ и  (см.стр \pageref{subsec:summ},  \pageref{MT}), принимающих значения $y_{INF,i}, y_{MT,i}\in\left[0,1\right]$, итоговая оценка вычисляется следующим образом:

\begin{equation}
\label{overall_score}
y_{overall,i} = \left(1-\lambda \right) y_{INF,i} +\lambda y_{MT,i}
\end{equation}
где $\lambda \in\left[0,1\right]$  - глобальный параметр контролирующий важность одной оценки - качество перевода, относительно другогой -  иформативность. После этого отбирается несколько предложений с наивысшими оценками. Количество отбираемых предлжений также настраиваемый параметр, зависящий от метрики ROUGE. После этого происходит перевод, описание см. в \cite{DBLP:journals/corr/KleinKDSR17}.
\bibliographystyle{plain}
\bibliography{Selnitskiy}
%\begin{thebibliography}{1}

%\bibitem{author09anyscience}
%    \BibAuthor{Author\;N.}
%    \BibTitle{Paper title}~//
%    \BibJournal{10-th Int'l. Conf. on Anyscience}, 2009.  Vol.\,11, No.\,1.  Pp.\,111--122.
%\bibitem{myHandbook}
%    \BibAuthor{Автор\;И.\,О.}
%    Название книги.
%    Город: Издательство, 2009. 314~с.
%\bibitem{author09first-word-of-the-title}
%    \BibAuthor{Автор\;И.\,О.}
%    \BibTitle{Название статьи}~//
%    \BibJournal{Название конференции или сборника},
%    Город:~Изд-во, 2009.  С.\,5--6.
%\bibitem{author-and-co2007}
%    \BibAuthor{Автор\;И.\,О., Соавтор\;И.\,О.}
%    \BibTitle{Название статьи}~//
%    \BibJournal{Название журнала}. 2007. Т.\,38, \No\,5. С.\,54--62.
%\bibitem{bibUsefulUrl}
%    \BibUrl{www.site.ru}~---
%    Название сайта.  2007.
%\bibitem{voron06latex}
%    \BibAuthor{Воронцов~К.\,В.}
%    \LaTeXe\ в~примерах.
%    2006.
%    \BibUrl{http://www.ccas.ru/voron/latex.html}.
%\bibitem{Lvovsky03}
%    \BibAuthor{Львовский~С.\,М.} Набор и вёрстка в пакете~\LaTeX.
%    3-е издание.
%    Москва:~МЦHМО, 2003.  448~с.
%\end{thebibliography}

% Решение Программного Комитета:
%\ACCEPTNOTE
%\AMENDNOTE
%\REJECTNOTE
\end{document}
